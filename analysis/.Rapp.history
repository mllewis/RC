n = length(unique(A3Loud$Item)) #
cutoff =  4/n
cutoff
Outlier stats#
alt.est_e3_2 <- estex(M6, "Item")#
data.frame(unique(A3Loud$Item), unique(A3Loud$words))#
# criterion = 4/n #
n = length(unique(A3Loud$Item)) #
cutoff =  4/n#
cooks_1_2 = ME.cook(alt.est_e3_2, sort=TRUE, plot=TRUE, cutoff= cutoff)
Outlier stats#
alt.est_e3_1 <- estex(model3, "Item")#
data.frame(unique(A3Loud$Item), unique(A3Loud$words))#
# criterion = 4/n #
n = length(unique(A3Loud$Item)) #
cutoff =  4/n#
 cooks_1_1 = ME.cook(alt.est_e3_1, sort=TRUE, plot=TRUE, cutoff= .2)
cooks_1_1
Outlier stats#
alt.est_e3_2 <- estex(M6, "Item")#
# criterion = 4/n #
n = length(unique(A3Loud$Item)) #
cutoff =  4/n#
cooks_1_2 = ME.cook(alt.est_e3_2, sort=TRUE, plot=TRUE, cutoff= cutoff)
Outlier stats#
alt.est_e3_1 <- estex(model3, "Item")#
# criterion = 4/n #
n = length(unique(A3Loud$Item)) #
cutoff =  4/n#
 cooks_1_1 = ME.cook(alt.est_e3_1, sort=TRUE, plot=TRUE, cutoff= cutoff)
cooks_1_2
2.024538e-01
M6_2 = lmer(Intensity ~ Loudsurvey + Gender+Syllables+Freq.logwf+(1|Subj) + (1|Item), REML=FALSE, data=A3Loud[A3Loud$Item != 'shrieking',]) #predicts with loud survey data#
summary(M6_2)
M5_2 = lmer(Intensity ~ Condition + Gender+Syllables+ Freq.logwf+(1|Subj) + (1|Item), REML=FALSE, data=A3Loud[A3Loud$Item != 'shrieking',]) #
summary(M5_2)#
pvals.fnc(M5_2)
M6_2 = lmer(Intensity ~ Loudsurvey + Gender+Syllables+Freq.logwf+(1|Subj) + (1|Item), REML=FALSE, data=A3Loud[A3Loud$Item != 'shrieking',]) #predicts with loud survey data#
summary(M6_2)#
pvals.fnc(M6_2)
M6 = lmer(Intensity ~ Loudsurvey + Gender+Syllables+Freq.logwf+(1|Subj) + (1|Item), REML=FALSE, data=A3Loud) #predicts with loud survey data#
summary(M6)
myTapply(A3Loud$Intensity,list(A3Loud$Loudsurvey),mean)
myTapply(A3Loud$LoudSurvey,list(A3Loud$Condition),mean)
myTapply(A3Loud$Loudsurvey,list(A3Loud$Condition),mean)
myTapply(A3Loud$Intensity,list(A3Loud$Loudsurvey),mean)
plot(myTapply(A3Loud$Intensity,list(A3Loud$Loudsurvey),mean))
cooks_1_1
cooks_1_2
Outlier stats -- Intensity#
alt.est_e3_2 <- estex(M5, "Item")#
# criterion = 4/n #
n = length(unique(A3Loud$Item)) #
cutoff =  4/n#
cooks_1_2 = ME.cook(alt.est_e3_2, sort=TRUE, plot=TRUE, cutoff= cutoff)#
#Shrieking
Intensity Models#
M5 = lmer(Intensity ~ Condition + Gender+Syllables+Freq.logwf+(1|Subj) + (1|Item), REML=FALSE, data=A3Loud) #predicts with condition#
summary(M5)#
pvals.fnc(M5)
alt.est_e3_2 <- estex(M5, "Item")
criterion = 4/n #
n = length(unique(A3Loud$Item)) #
cutoff =  4/n#
cooks_1_2 = ME.cook(alt.est_e3_2, sort=TRUE, plot=TRUE, cutoff= cutoff)
cooks_1_2
M6 = lmer(Intensity ~ Loudsurvey + Gender+Syllables+Freq.logwf+(1|Subj) + (1|Item), REML=FALSE, data=A3Loud) #predicts with loud survey data#
summary(M6)
Outlier stats -- Intensity - loudness#
 alt.est_e3_3 <- estex(M6, "Item")
criterion = 4/n #
n = length(unique(A3Loud$Item)) #
cutoff =  4/n#
cooks_1_3 = ME.cook(alt.est_e3_3, sort=TRUE, plot=TRUE, cutoff= cutoff)
cooks_1_3
M6_2 = lmer(Intensity ~ Loudsurvey + Gender+Syllables+Freq.logwf+(1|Subj) + (1|Item), REML=FALSE, data=A3Loud[A3Loud$Item != 'shrieking',]) #predicts with loud survey data#
summary(M6_2)#
pvals.fnc(M6_2)
cooks_3
data.frame(unique(d$Item), unique(d$words))
cooks_1
cooks_1_1
cooks_2
alt.est_e2 <- estex(IntensityS1, "Item")
summary(IntensityS1)
library(languageR)#
source("/MISC/lmercompare.R")#
source("/Documents/CaLL/from my laptop/Misc/myTapply.R")#
#
rm(graph)#
##write.table(Activity,file="A4withFIllers.csv",sep=",",row.names=T)#
#
## READ DATASET#
Activity= read.csv("/Documents/CaLL/from work computer/Experiments/Activity/Data_Organized/Experiment #4/Analysis/ALL (1-63).csv")#
#fillers= read.csv("/Documents/CaLL/from work computer/Experiments/Activity/Data_Organized/Experiment #4/Analysis/ALL fillers.csv")#
#fillers$Filler = 'F'#
#
##Bind into 1 dataframe#
#Activity <- rbind(critical, fillers)#
#summary(Activity)#
#ADD Semantics3#
Activity$Semantics3 <- ifelse((Activity$Semantics == 'P' | Activity$Semantics2 == 'NonVocal' ), "NV", "V")#
summary(Activity)#
#
#ADD Semantics4#
Activity$Semantics4 <- #
	ifelse((Activity$Semantics == 'V') ,#
	 "V", ifelse(Activity$Semantics2 == "NonVocal", "NV"))#
summary(Activity)#
#
# MAKE FACTORS#
Activity$Subject = factor(Activity$Subject)#
Activity$Semantics = factor(Activity$Semantics)#
Activity$Semantics3 = factor(Activity$Semantics3)#
Activity$Semantics4 = factor(Activity$Semantics4)#
#
Activity$Picture = factor(Activity$Picture) #
Activity$Item = factor(Activity$Item)#
#Activity$Semantics2 <- factor(Activity$Semantics2,levels=c('NonVocal','Foot','LowVocal','HighVocal')) # reorders#
Activity$Semantics2 <- factor(Activity$Semantics2,levels=c('Foot','LowVocal','HighVocal')) # reorders
SET CONTRASTS#
#Semantics and Semantics2#
contrasts(Activity$Semantics) = c(-.5,.5)#
contrasts(Activity$Semantics) # check#
contrasts(Activity$Semantics2) = contr.helmert#
contrasts(Activity$Semantics2) # check#
#Picture#
#Activity$Picture <- factor(Activity$Picture,levels=c('S','T','M')) # reorders#
#contrasts(Activity$Picture) = contr.helmert#
#contrasts(Activity$Picture) # check#
#
#Subject Gender#
counttotal = length(Activity$Gender)#
countM = length(Activity$Gender[Activity$Gender=='M'])#
countF = length(Activity$Gender[Activity$Gender=='F'])#
weightF = countM/counttotal#
weightM = countF/counttotal#
contrasts(Activity$Gender) <- cbind(c(weightF,-1*weightM))#
contrasts(Activity$Gender) # display to check
IntensityS1 = lmer(TargetIntensity ~ Semantics2 +(1|Subject) + (1|Item), REML=FALSE, data=Activity)#
summary(IntensityS1)
criterion = 4/n #
n = length(unique(Activity$Item)) #
cutoff =  4/n
cooks_2= ME.cook(alt.est_e2, sort=TRUE, plot=TRUE, cutoff= cutoff)
cooks_2
cutoff
model3 = lmer(TargetMin ~ Condition + Gender+Syllables+(1|Subj) + (1|Item), REML=FALSE, data=A3Loud) #
summary(model3)
model3_2 = lmer(TargetMin ~ Condition + Gender+Syllables+(1|Subj) + (1|Item), REML=FALSE, data=A3Loud[A3Loud$Item != 'chatting',]) #
summary(model3_2)
pvals.fnc(model3_2)
alt.est_e3_2 <- estex(M5, "Subject")
summary(M5)
alt.est_e3_3 <- estex(M5, "Subj")
n = length(unique(A3Loud$subj))
n
unique(A3Loud$subj)
n = length(unique(A3Loud$Subj))
n
cutoff =  4/n
cooks_1_3 = ME.cook(alt.est_e3_3, sort=TRUE, plot=TRUE, cutoff= cutoff)
cooks_1_3
cuttoff
cutoff
M5_2 = lmer(Intensity ~ Condition + Gender+Syllables+ Freq.logwf+(1|Subj) + (1|Item), REML=FALSE, data=A3Loud[A3Loud$Item != 'shrieking',]) #
summary(M5_2)
pvals.fnc(M5_2)
M6_2 = lmer(Intensity ~ Loudsurvey + Gender+Syllables+Freq.logwf+(1|Subj) + (1|Item), REML=FALSE, data=A3Loud[A3Loud$Item != 'shrieking',]) #predicts with loud survey data#
summary(M6_2)#
pvals.fnc(M6_2)
d = read.table('/Documents/GRADUATE_SCHOOL/Coursework/Psych. 253/HW3/personality0.txt')
summary(d)
head(d)
dd = as.dist((1 - cor(d))/2)		#matrix of 'distances' between variables
dd
rs1 = hclust(dd)
rs1
d0 = read.table("crimestates0.txt", header=F)
res1b = factanal(d, factors=10, rotation="none",na.action=na.omit)#
r
res1b
res1b = factanal(d, factors=10, rotation="none",na.action=na.omit)
res1b$loadings
res1b = factanal(d, factors=10, rotation="none",na.action=na.omit)
ar(mfrow=c(1,2))#
variables = names(d)#
plot(res1b$loadings, type='n')#
text(res1b$loadings, labels=variables, cex=.5)#
plot(res1a$loadings, type='n')#
text(res1a$loadings, labels=variables, cex=.5)
Plot loadings against one another#
par(mfrow=c(1,2))#
variables = names(d)#
plot(res1b$loadings, type='n')#
text(res1b$loadings, labels=variables, cex=.5)#
plot(res1a$loadings, type='n')#
text(res1a$loadings, labels=variables, cex=.5)
Factor analysis with no rotation - ~6 good factors. Interpretable?#
res1b = factanal(d, factors=10, rotation="none",na.action=na.omit)#
res1b$loadings#
#
###Factor analysis with rotation. ~7 factors, interpretable?#
res1a = factanal(d, factors=10, na.action=na.omit)#
res1a$loadings#
#
###Plot loadings against one another#
par(mfrow=c(1,2))#
variables = names(d)#
plot(res1b$loadings, type='n')#
text(res1b$loadings, labels=variables, cex=.5)#
plot(res1a$loadings, type='n')#
text(res1a$loadings, labels=variables, cex=.5)
pdf('plot1.pdf')#
par(mfrow=c(1,2))#
variables = names(d)#
plot(res1b$loadings, type='n')#
text(res1b$loadings, labels=variables, cex=.5)#
plot(res1a$loadings, type='n')#
text(res1a$loadings, labels=variables, cex=.5)#
dev.off()
shyness = d$distant+d$shy+d$withdrw+d$quiet#
outgoingness = d$talkatv + d$outgoin#
hardworkingness = d$hardwrk + d$persevr#
#etc, you guys choose what you want to combine#
combined_data = cbind(shyness,outgoingness,hardworkingness)#
combined_data = as.data.frame(combined_data)#
res2 = factanal(combined_data, factors = 1, na.action=na.omit)#
res2$loadings
PCA without rotation#
pca1 = principal(d, nfactors=2, rotate="none")#
pca1
library(psych)#
#
###PCA without rotation#
pca1 = principal(d, nfactors=2, rotate="none")#
pca1
Plot loadings against one another#
par(mfrow=c(1,2))#
variables = names(d)#
plot(pca1$loadings, type='n')#
text(pca1$loadings, labels=variables, cex=.9)#
plot(pca2$loadings, type='n')#
text(pca2$loadings, labels=variables, cex=.9)
Plot loadings against one another#
par(mfrow=c(1,2))#
variables = names(d)#
plot(pca1$loadings, type='n')#
text(pca1$loadings, labels=variables, cex=.9)#
plot(pca2$loadings, type='n')#
text(pca2$loadings, labels=variables, cex=.9)#
#
###Hierarchical cluster analysis#
par(mfrow=c(1,2))#
dd = as.dist((1-cor(d))/2) ##Dissimilarity matrix#
rs1=hclust(dd)#
rs1$merge#
plot(rs1$height)#
plot(rs1)
Plot loadings against one another#
par(mfrow=c(1,2))#
variables = names(d)#
plot(pca1$loadings, type='n')#
text(pca1$loadings, labels=variables, cex=.9)#
plot(pca2$loadings, type='n')#
text(pca2$loadings, labels=variables, cex=.9)#
#
###Hierarchical cluster analysis#
par(mfrow=c(1,2))#
dd = as.dist((1-cor(d))/2) ##Dissimilarity matrix#
rs1=hclust(dd)#
rs1$merge#
plot(rs1$height, main='hierarchical cluster analysis')#
plot(rs1)
Plot loadings against one another#
pdf("plot2.pdf")#
par(mfrow=c(1,2))#
variables = names(d)#
plot(pca1$loadings, type='n')#
text(pca1$loadings, labels=variables, cex=.9)#
plot(pca2$loadings, type='n')#
text(pca2$loadings, labels=variables, cex=.9)#
#
###Hierarchical cluster analysis#
par(mfrow=c(1,2))#
dd = as.dist((1-cor(d))/2) ##Dissimilarity matrix#
rs1=hclust(dd)#
rs1$merge#
plot(rs1$height, main='hierarchical cluster analysis')#
plot(rs1)#
dev.off()
par(mfrow=c(1,2))#
variables = names(d)#
plot(pca1$loadings, type='n')#
text(pca1$loadings, labels=variables, cex=.9)#
plot(pca2$loadings, type='n')#
text(pca2$loadings, labels=variables, cex=.9)#
#
###Hierarchical cluster analysis#
par(mfrow=c(1,2))#
dd = as.dist((1-cor(d))/2) ##Dissimilarity matrix#
rs1=hclust(dd)#
rs1$merge#
plot(rs1$height, main='hierarchical cluster analysis')#
plot(rs1)
pdf("plot2.pdf")#
par(mfrow=c(1,2))#
variables = names(d)#
plot(pca1$loadings, type='n')#
text(pca1$loadings, labels=variables, cex=.9)#
plot(pca2$loadings, type='n')#
text(pca2$loadings, labels=variables, cex=.9)#
#
###Hierarchical cluster analysis#
par(mfrow=c(1,2))#
dd = as.dist((1-cor(d))/2) ##Dissimilarity matrix#
rs1=hclust(dd)#
rs1$merge#
plot(rs1$height, main='hierarchical cluster analysis')#
plot(rs1)#
dev.off()
Plot loadings against one another#
pdf("plot2.pdf")#
variables = names(d)#
plot(pca1$loadings, type='n')#
text(pca1$loadings, labels=variables, cex=.9)#
plot(pca2$loadings, type='n')#
text(pca2$loadings, labels=variables, cex=.9)#
dev.off()#
###Hierarchical cluster analysis#
pdf("plot3.pdf")#
dd = as.dist((1-cor(d))/2) ##Dissimilarity matrix#
rs1=hclust(dd)#
rs1$merge#
plot(rs1$height, main='hierarchical cluster analysis')#
plot(rs1)#
dev.off()
Plot loadings against one another#
pdf("plot2.pdf")#
variables = names(d)#
plot(pca1$loadings, type='n')#
text(pca1$loadings, labels=variables, cex=.9)#
plot(pca2$loadings, type='n')#
text(pca2$loadings, labels=variables, cex=.9)#
dev.off()
PCA without rotation#
pca1 = principal(d, nfactors=2, rotate="none")#
pca1#
#
###PCA with rotation#
pca2 = principal(d, nfactors=2, rotate="varimax")#
pca2#
#
###Plot loadings against one another#
pdf("plot2.pdf")#
variables = names(d)#
plot(pca1$loadings, type='n')#
text(pca1$loadings, labels=variables, cex=.9)#
plot(pca2$loadings, type='n')#
text(pca2$loadings, labels=variables, cex=.9)#
dev.off()
Hierarchical cluster analysis#
pdf("plot3.pdf")#
dd = as.dist((1-cor(d))/2) ##Dissimilarity matrix#
rs1=hclust(dd)#
rs1$merge#
plot(rs1$height, main='hierarchical cluster analysis')#
plot(rs1)#
dev.off()
dd = as.dist((1-cor(d))/2) ##Dissimilarity matrix#
rs1=hclust(dd)#
rs1$merge#
plot(rs1$height, main='hierarchical cluster analysis')#
plot(rs1)
dev.off()
dd = as.dist((1-cor(d))/2) ##Dissimilarity matrix#
rs1=hclust(dd)#
rs1$merge#
plot(rs1$height, main='hierarchical cluster analysis')#
plot(rs1)
pdf("plot3.pdf")#
dd = as.dist((1-cor(d))/2) ##Dissimilarity matrix#
rs1=hclust(dd)#
rs1$merge#
plot(rs1$height, main='hierarchical cluster analysis')#
plot(rs1)#
dev.off()
variables = names(d)#
plot(pca1$loadings, type='n')#
text(pca1$loadings, labels=variables, cex=.9)#
plot(pca2$loadings, type='n')#
text(pca2$loadings, labels=variables, cex=.9)
pdf("plot2.pdf")#
variables = names(d)#
plot(pca1$loadings, type='n')#
text(pca1$loadings, labels=variables, cex=.9)#
plot(pca2$loadings, type='n')#
text(pca2$loadings, labels=variables, cex=.9)#
dev.off()
Plot loadings against one another#
pdf("plot_problem2.pdf")#
variables = names(d)#
plot(pca1$loadings, type='n')#
text(pca1$loadings, labels=variables, cex=.9)#
plot(pca2$loadings, type='n')#
text(pca2$loadings, labels=variables, cex=.9)#
dev.off()
Hierarchical cluster analysis#
pdf("plot3.pdf")#
par(mfrow=c(1,2))#
dd = as.dist((1-cor(d))/2) ##Dissimilarity matrix#
rs1=hclust(dd)#
rs1$merge#
plot(rs1$height, main='hierarchical cluster analysis')#
plot(rs1)#
dev.off()
Plot loadings against one another#
pdf("plot_problemblah.pdf")#
par(mfrow=c(1,2))#
#
variables = names(d)#
plot(pca1$loadings, type='n')#
text(pca1$loadings, labels=variables, cex=.9)#
par(mfrow=c(2,2))#
plot(pca2$loadings, type='n')#
text(pca2$loadings, labels=variables, cex=.9)#
dev.off()
Plot loadings against one another#
pdf("plot_problemb.pdf")#
par(mfrow=c(1,2))#
#
variables = names(d)#
plot(pca1$loadings, type='n')#
text(pca1$loadings, labels=variables, cex=.9)#
plot(pca2$loadings, type='n')#
text(pca2$loadings, labels=variables, cex=.9)#
dev.off()
Hierarchical cluster analysis#
pdf("plot_problemc.pdf")#
dd = as.dist((1-cor(d))/2) ##Dissimilarity matrix#
rs1=hclust(dd)#
rs1$merge#
plot(rs1$height, main='hierarchical cluster analysis')#
plot(rs1)#
dev.off()
Hierarchical cluster analysis#
pdf("plot_problemc.pdf")#
par(mfrow=c(1,2))#
#
dd = as.dist((1-cor(d))/2) ##Dissimilarity matrix#
rs1=hclust(dd)#
rs1$merge#
plot(rs1$height, main='hierarchical cluster analysis')#
plot(rs1)#
dev.off()
Hierarchical cluster analysis#
pdf("plot_problemc.pdf")#
dd = as.dist((1-cor(d))/2) ##Dissimilarity matrix#
rs1=hclust(dd)#
rs1$merge#
dev.off()#
#
plot(rs1$height, main='hierarchical cluster analysis')#
pdf("plot_problemd.pdf")#
#
plot(rs1)#
dev.off()
Factor analysis with no rotation - ~6 good factors. Interpretable?#
res1b = factanal(d, factors=10, rotation="none",na.action=na.omit)#
res1b$loadings
res1a = factanal(d, factors=10, na.action=na.omit)#
res1a$loadings
Plot loadings against one another#
pdf('plot1.pdf')#
par(mfrow=c(1,2))#
variables = names(d)#
plot(res1b$loadings, type='n', main = "unrotated")#
text(res1b$loadings, labels=variables, cex=.5)#
plot(res1a$loadings, type='n', main = "rotated")#
text(res1a$loadings, labels=variables, cex=.5)#
dev.off()
res1a$loadings
names(d)
shyness = d$distant+d$shy+d$withdrw+d$quiet#
outgoingness = d$talkatv + d$outgoin#
hardworkingness = d$hardwrk + d$persevr#
organized = d$organiz+d$discipl+d$respnsi#
unmotivated = d$carelss+d$lazy+d$givinup#
friendly = d$agreebl+d$kind+d$friendl+d$coopera#
chill = d$laidbck+d$easygon+d$relaxed#
jerk = d$criticl+d$contrar+d$harsh+d$opposng#
uptight = d$tense+d$anxious+d$worryin#
combined_data = cbind(shyness,outgoingness,hardworkingness, organize,unmotivated, friendly, chill, jerk, uptight)
combined_data = cbind(shyness,outgoingness,hardworkingness, organized,unmotivated, friendly, chill, jerk, uptight)
combined_data = as.data.frame(combined_data)
res2 = factanal(combined_data, factors = 1, na.action=na.omit)
res2$loadings
shyness = d$distant+d$shy+d$withdrw+d$quiet#
outgoingness = d$talkatv + d$outgoin#
hardworkingness = d$hardwrk + d$persevr#
organized = d$organiz+d$discipl+d$respnsi#
unmotivated = d$carelss+d$lazy+d$givinup#
friendly = d$agreebl+d$kind+d$friendl+d$coopera#
chill = d$laidbck+d$easygon+d$relaxed#
difficult = d$criticl+d$contrar+d$harsh+d$opposng#
uptight = d$tense+d$anxious+d$worryin#
combined_data = cbind(shyness,outgoingness,hardworkingness, organized,unmotivated, friendly, chill, difficult, uptight)#
combined_data = as.data.frame(combined_data)#
res2 = factanal(combined_data, factors = 1, na.action=na.omit)#
res2$loadings
res2 = factanal(combined_data, factors = 10, na.action=na.omit)#
res2$loadings
res2 = factanal(combined_data, factors = 9, na.action=na.omit)#
res2$loadings
res2 = factanal(combined_data, factors = 5, na.action=na.omit)#
res2$loadings
plot(rs1)
pdf("plot_problemd.pdf")#
#
plot(rs1)#
dev.off()
pdf("plot_problemd.pdf")#
plot(rs1$height, main='hierarchical cluster analysis')#
dev.off()
pdf("plot_probleme.pdf")#
plot(rs1)#
dev.off()
plot(rs1)
plot(rs1$height, main='hierarchical cluster analysis')
dd = as.dist((1-cor(d))/2) ##Dissimilarity matrix#
rs1=hclust(dd)#
rs1$merge
library(psych)
d = read.csv("person.option2.vars.csv")#
d0 = d[, c(1:32)] #data for factor analysis#
d1 = d[, c(33:37)] ## data for later step
d = read.csv("person.option2.vars.csv")
d = read.csv('/Documents/GRADUATE_SCHOOL/Coursework/Psych. 253/HW3/person.option3.hw.csv')
d = read.csv("person.option2.vars.csv")
d= read.csv('/Documents/GRADUATE_SCHOOL/Coursework/Psych. 253/HW3/person.option2.vars.csv')
d = read.csv("person.option2.vars.csv")
d= read.csv('/Documents/GRADUATE_SCHOOL/Coursework/Psych. 253/HW3/person.option2.vars.csv')
summary(d)
d0 = d[, c(1:32)] #data for factor analysis#
d1 = d[, c(33:37)] ## data for later step
step 1: do a factor analysis#
res1 = factanal(d0, factors=4, rotation="varimax",na.action=na.omit, scores= 'r')#
print(res1$loadings, cutoff = .4)		#Use loadings to interpret PC's#
res1$scores
d10 = mlogit.data(d1, choice = "rating2", varying = c(2:5), shape = "wide") #"varying" corresponds to variables that are alternative specfic: more on that later
head(d1) #
head(d10)
library(mlogit)
res1 = factanal(d0, factors=4, rotation="varimax",na.action=na.omit, scores= 'r')#
print(res1$loadings, cutoff = .4)		#Use loadings to interpret PC's#
res1$scores#
#
#add loadings to d1 for use in the next step#
d1$PC1 = res1$scores[,1]; d1$PC2 = -res1$scores[,2]#
d1$PC3 = res1$scores[,3]; d1$PC4 = res1$scores[,4]#
#
#let's see if we can predict "rating2" from our PC's!#
d1$rating2 = factor(d1$rating2, labels = c('A', 'B'))	#let's make our rating a factor first
step 1: do a factor analysis#
res1 = factanal(d0, factors=4, rotation="varimax",na.action=na.omit, scores= 'r')#
print(res1$loadings, cutoff = .4)		#Use loadings to interpret PC's#
res1$scores#
#
#add loadings to d1 for use in the next step#
d1$PC1 = res1$scores[,1]; d1$PC2 = -res1$scores[,2]#
d1$PC3 = res1$scores[,3]; d1$PC4 = res1$scores[,4]
step 1: do a factor analysis#
res1 = factanal(d0, factors=4, rotation="varimax",na.action=na.omit, scores= 'r')#
print(res1$loadings, cutoff = .4)		#Use loadings to interpret PC's#
res1$scores#
#
#add loadings to d1 for use in the next step#
d1$PC1 = res1$scores[,1]; d1$PC2 = -res1$scores[,2]#
d1$PC3 = res1$scores[,3]; d1$PC4 = res1$scores[,4]#
#
#let's see if we can predict "rating2" from our PC's!#
d1$rating2 = factor(d1$rating2, labels = c('A', 'B'))	#let's make our rating a factor first#
#
#before we can do that though we need to put the data in a certain form. Luckily mlogit has a built in function to do that!#
d10 = mlogit.data(d1, choice = "rating2", varying = c(2:5), shape = "wide") #"varying" corresponds to variables that are alternative specfic: more on that later#
#
#Let's compare the old data and the new#
head(d1) #
head(d10)
d= read.csv('/Documents/GRADUATE_SCHOOL/Coursework/Psych. 253/HW3/person.option2.vars.csv')#
d0 = d[, c(1:32)] #data for factor analysis#
d1 = d[, c(33:37)] ## data for later step#
#
###step 1: do a factor analysis#
res1 = factanal(d0, factors=4, rotation="varimax",na.action=na.omit, scores= 'r')#
print(res1$loadings, cutoff = .4)		#Use loadings to interpret PC's#
res1$scores
add loadings to d1 for use in the next step#
d1$PC1 = res1$scores[,1]; d1$PC2 = -res1$scores[,2]#
d1$PC3 = res1$scores[,3]; d1$PC4 = res1$scores[,4]
let's see if we can predict "rating2" from our PC's!#
d1$rating2 = factor(d1$rating2, labels = c('A', 'B'))	#let's make our rating a factor first
before we can do that though we need to put the data in a certain form. Luckily mlogit has a built in function to do that!#
d10 = mlogit.data(d1, choice = "rating2", varying = c(2:5), shape = "wide") #"varying" corresponds to variables that are alternative specfic: more on that later
library(mlogit)
? mlogit.data
d10 = mlogit.data(d1, choice = "rating2", varying = c(2:5), shape = "wide") #"varying" corresponds to variables that are alternative specfic: more on that later
library(mlogit)
?mlogit
before we can do that though we need to put the data in a certain form. Luckily mlogit has a built in function to do that!#
d10 = mlogit.data(d1, choice = "rating2", varying = c(2:5), shape = "wide") #"varying" corresponds to variables that are alternative specfic: more on that later
interaction.plot(d1$)
summary(d1$rating2)
summary(d1$attribute2)
summary(d10$attribute2)
library(mlogit)#
#
d= read.csv('/Documents/GRADUATE_SCHOOL/Coursework/Psych. 253/HW3/person.option2.vars.csv')#
d0 = d[, c(1:32)] #data for factor analysis#
d1 = d[, c(33:37)] ## data for later step#
#
###step 1: do a factor analysis#
res1 = factanal(d0, factors=4, rotation="varimax",na.action=na.omit, scores= 'r')#
print(res1$loadings, cutoff = .4)		#Use loadings to interpret PC's#
res1$scores#
#
#add loadings to d1 for use in the next step#
d1$PC1 = res1$scores[,1]; d1$PC2 = -res1$scores[,2]#
d1$PC3 = res1$scores[,3]; d1$PC4 = res1$scores[,4]#
#
#let's see if we can predict "rating2" from our PC's!#
d1$rating2 = factor(d1$rating2, labels = c('A', 'B'))	#let's make our rating a factor first#
#
#before we can do that though we need to put the data in a certain form. Luckily mlogit has a built in function to do that!#
d10 = mlogit.data(d1, choice = "rating2", varying = c(2:5), shape = "wide") #"varying" corresponds to variables that are alternative specfic: more on that later
d1 = read.csv("person.option3.hw.csv")
d1 = read.csv("/Documents/GRADUATE_SCHOOL/Coursework/Psych. 253/HW3/person.option3.hw.csv")
d10 = mlogit.data(d1, choice = "rating3need", varying = c(6:8), shape = "wide")
Factor analysis with rotation. ~7 factors, interpretable?#
res1a = factanal(d, factors=10, na.action=na.omit)#
res1a$loadings
QUESTION 1#
d = read.table('/Documents/GRADUATE_SCHOOL/Coursework/Psych. 253/HW3/personality0.txt')#
#
###Factor analysis with no rotation - ~6 good factors. Interpretable?#
res1b = factanal(d, factors=10, rotation="none",na.action=na.omit)#
res1b$loadings#
#
###Factor analysis with rotation. ~7 factors, interpretable?#
res1a = factanal(d, factors=10, na.action=na.omit)#
res1a$loadings#
#
###Plot loadings against one another#
pdf('plot1.pdf')#
par(mfrow=c(1,2))#
variables = names(d)#
plot(res1b$loadings, type='n', main = "unrotated")#
text(res1b$loadings, labels=variables, cex=.5)#
plot(res1a$loadings, type='n', main = "rotated")#
text(res1a$loadings, labels=variables, cex=.5)#
dev.off()#
#
###Combining synonyms. For example...#
#
shyness = d$distant+d$shy+d$withdrw+d$quiet#
outgoingness = d$talkatv + d$outgoin#
hardworkingness = d$hardwrk + d$persevr#
organized = d$organiz+d$discipl+d$respnsi#
unmotivated = d$carelss+d$lazy+d$givinup#
friendly = d$agreebl+d$kind+d$friendl+d$coopera#
chill = d$laidbck+d$easygon+d$relaxed#
difficult = d$criticl+d$contrar+d$harsh+d$opposng#
uptight = d$tense+d$anxious+d$worryin#
#
combined_data = cbind(shyness,outgoingness,hardworkingness, organized,unmotivated, friendly, chill, difficult, uptight)#
combined_data = as.data.frame(combined_data)#
res2 = factanal(combined_data, factors = 5, na.action=na.omit)#
res2$loadings#
#
###QUESTION 2#
library(psych)#
#
###PCA without rotation#
pca1 = principal(d, nfactors=2, rotate="none")#
pca1#
#
###PCA with rotation#
pca2 = principal(d, nfactors=2, rotate="varimax")#
pca2#
#
###Plot loadings against one another#
pdf("plot_problemb.pdf")#
par(mfrow=c(1,2))#
variables = names(d)#
plot(pca1$loadings, type='n')#
text(pca1$loadings, labels=variables, cex=.9)#
plot(pca2$loadings, type='n')#
text(pca2$loadings, labels=variables, cex=.9)#
dev.off()#
#
###Hierarchical cluster analysis#
dd = as.dist((1-cor(d))/2) ##Dissimilarity matrix#
rs1=hclust(dd)#
rs1$merge#
#
pdf("plot_problemd.pdf")#
plot(rs1$height, main='hierarchical cluster analysis')#
dev.off()#
#
pdf("plot_probleme.pdf")#
plot(rs1)#
dev.off()#
#
###QUESTION 3 (Optional)#
d = read.table("crimestates0.txt", header=F)#
cnames1 = c('state','VC.rate','M.rate','met.res','pc.white','HS.grads','pov.rate','s.parent')#
colnames(d) = cnames1#
statenames = d[,1] #extract state names for use in naming columns below#
d1 = scale(d[,-1]) #standardise quant variables because of differing scales#
d2 = t(as.matrix(d1)) #transpose numerical data to 'columns' as 'states'#
colnames(d2) = statenames #name the cols after states#
par(mfrow=c(2,1))#
dd = as.dist((1 - cor(d2))/2) #matrix of 'distances' between states#
dend = hclust(dd)#
plot(dend$height)#
plot(dend, cex = .5)
â€¦
log(.417/.296)
LOAD REQUIRED PAGACKES#
library(languageR)#
source("/MISC/lmercompare.R")#
source("/Users/languagelab/Downloads/Activity Presentation Graphs/myTapply.R")#
#
## READ DATA SET#
A3= read.csv("/Documents/CaLL/from work computer/Experiments/Activity/Data_Organized/Experiment #3/Analysis/A3Results(1-38)c.csv")#
summary(A3)#
#
#SUBSET DATA INTO LOUDNESS ONLY#
A3Loud=subset(A3, A3$Condition=="V"|A3$Condition=="LV")#
summary(A3Loud)#
#
#MAKE FACTORS#
A3Loud$Subj=factor(A3Loud$Subj.)	#Subject#
A3Loud$Condition=factor(A3Loud$Condition) #Condition#
summary(A3Loud)#
#
#DELETE USELESS VARIABLES#
A3Loud$Subj. <-NULL#
A3Loud$Subject <-NULL#
A3Loud$Filename <-NULL#
A3Loud$subjitem <-NULL#
summary(A3Loud)#
#
# SET CONTRASTS#
#Condition#
contrasts(A3Loud$Condition) = c(-.5,.5)#
contrasts(A3Loud$Condition)  #check#
#
#SubjectGender#
counttotal = length(A3Loud$Gender)#
countM = length(A3Loud$Gender[A3Loud$Gender=='M'])#
countF = length(A3Loud$Gender[A3Loud$Gender=='F'])#
weightF = countM/counttotal#
weightM = countF/counttotal#
#
contrasts(A3Loud$Gender) <- cbind(c(weightF,-1*weightM))#
contrasts(A3Loud$Gender) # display to check
RUN TARGET MIN MODELS#
# CONDITION#
MConditiononly = lmer(TargetMin ~ Condition + (1|Subj) + (1|Item), REML=FALSE, data=A3Loud) #
summary(MConditiononly)#
#
MConditiononlys = lmer(TargetMin ~ Condition + (1 + Condition|Subj) + (1|Item), REML=FALSE, data=A3Loud) #
summary(MConditiononlys)
model3 = lmer(TargetMin ~ Condition + Gender+Syllables+(1|Subj) + (1|Item), REML=FALSE, data=A3Loud) #
summary(model3)
Outlier stats - targetMin#
alt.est_e3_1 <- estex(model3, "Item")#
# criterion = 4/n #
n = length(unique(A3Loud$Item)) #
cutoff =  4/n#
 cooks_1_1 = ME.cook(alt.est_e3_1, sort=TRUE, plot=TRUE, cutoff= cutoff)#
 # chatting
model3_2 = lmer(TargetMin ~ Condition + Gender+Syllables+(1|Subj) + (1|Item), REML=FALSE, data=A3Loud[A3Loud$Item != 'chatting',]) #
summary(model3_2)#
pvals.fnc(model3_2)
cooks1_1
cooks_1_1
2.024538e-01
modelL3 = lmer(TargetMin ~ Loudsurvey + Gender+Syllables+(1|Subj) + (1|Item), REML=FALSE, data=A3Loud) #
summary(modelL3)
Outlier stats -- Intensity - condition#
alt.est_IntC <- estex(M5, "Item")#
# criterion = 4/n #
n = length(unique(A3Loud$Item)) #
cutoff =  4/n#
cooks_1_IntC = ME.cook(alt.est_IntC, sort=TRUE, plot=TRUE, cutoff= cutoff)#
#Shrieking
cooks_1_IntC
2.142240e-01
Outlier stats -- Intensity - loudness#
 alt.est_IntS <- estex(M6, "Item")#
# criterion = 4/n #
n = length(unique(A3Loud$Item)) #
cutoff =  4/n#
cooks_1_IntS = ME.cook(alt.est_IntS, sort=TRUE, plot=TRUE, cutoff= cutoff)#
#Shrieking
cooks_1_IntS
Intensity Models#
M5 = lmer(Intensity ~ Condition + Gender+Syllables+Freq.logwf+(1|Subj) + (1|Item), REML=FALSE, data=A3Loud) #predicts with condition#
summary(M5)#
pvals.fnc(M5)
M5_2 = lmer(Intensity ~ Condition + Gender+Syllables+ Freq.logwf+(1|Subj) + (1|Item), REML=FALSE, data=A3Loud[A3Loud$Item != 'shrieking',]) #
summary(M5_2)#
pvals.fnc(M5_2)
M6_2 = lmer(Intensity ~ Loudsurvey + Gender+Syllables+Freq.logwf+(1|Subj) + (1|Item), REML=FALSE, data=A3Loud[A3Loud$Item != 'shrieking',]) #predicts with loud survey data#
summary(M6_2)#
pvals.fnc(M6_2)
library('mlogit')
library(zoo)
?r
R.version()
R.Version()
d = read.csv("/Documents/GRADUATE_SCHOOL/Coursework/Psych. 253/HW4/person.choice3.csv")
summary(d)
library('mlogit')
library('zoo')
y
res4 = mlogit(choice3 ~ consci, data = d2)#
summary(res4)
res2 =mlogit(choice3 ~ price | extrav + consci , data = d2) #estimating 1 price coefficient for all choicees...#
summary(res2)
library(lavaan)
d = read.csv("jt-data1.csv")
d = read.csv("/Documents/GRADUATE_SCHOOL/Coursework/Psych. 253/HW7/savt1.r/jt-data1.csv")
d = read.csv("/Documents/GRADUATE_SCHOOL/Coursework/Psych. 253/HW7/jt-data1.csv")
avt.model1 = '#
	# regressions#
	# X + Y ~ U + V + W#
	ideahap + actuhap ~ cultatt + temperatt#
	rigoract + depress ~ ideahap + actuhap + cultatt + temperatt#
	# variances and covariances#
	# X ~~ Y#
	rigoract ~~ 0*depress#
'
summary(d)
M1###
#
# Fit the model#
avt.fit1 = sem(avt.model1, fixed.x = F, data = d)
summary(avt.fit1)
avt.model2 = '#
	# regressions#
	# X + Y ~ U + V + W#
	ideahap + actuhap ~ cultatt + temperatt#
	rigoract ~ ideahap + temperatt#
	depress ~ actuhap + temperatt#
	# variances and covariances#
	# X ~~ Y#
	rigoract ~~ 0*depress#
	ideahap ~~ actuhap#
'
avt.fit2 = sem(avt.model2, fixed.x = F, data = d)
print(summary(avt.fit2))
grplab1 = c("EA", "AA", "CH")#
#
# Fit Model 2 to all 3 groups, params differ across groups#
#
avt.group1 = sem(avt.model2, fixed.x = F, data = d, group = "group", meanstructure = F)
summary(avt.group1)
avt.group2 = sem(avt.model2, fixed.x = F, data = d, group = "group", meanstructure = F, group.equal = "regressions")
print(summary(avt.group2))
avt.fit1 = sem(avt.model1, fixed.x = F, data = d)#
#
summary(avt.fit1)
avt.fit1 = sem(avt.model1, fixed.x = F, data = d)#
avt.fit1
avt.model2 = '#
	# regressions#
	# X + Y ~ U + V + W#
	ideahap + actuhap ~ cultatt + temperatt#
	rigoract ~ ideahap + temperatt#
	depress ~ actuhap + temperatt#
	# variances and covariances#
	# X ~~ Y#
	rigoract ~~ 0*depress#
	ideahap ~~ actuhap#
'#
# Fit the model#
avt.fit2 = sem(avt.model2, fixed.x = F, data = d)#
summary(avt.fit2))
avt.model2 = '#
	# regressions#
	# X + Y ~ U + V + W#
	ideahap + actuhap ~ cultatt + temperatt#
	rigoract ~ ideahap + temperatt#
	depress ~ actuhap + temperatt#
	# variances and covariances#
	# X ~~ Y#
	rigoract ~~ 0*depress#
	ideahap ~~ actuhap#
'#
# Fit the model#
avt.fit2 = sem(avt.model2, fixed.x = F, data = d)#
summary(avt.fit2)
grplab1 = c("EA", "AA", "CH")#
#
# Fit Model 2 to all 3 groups, params differ across groups#
#
avt.group1 = sem(avt.model2, fixed.x = F, data = d, group = "group", meanstructure = F)#
#
summary(avt.group1)
avt.group2 = sem(avt.model2, fixed.x = F, data = d, group = "group", meanstructure = F, group.equal = "regressions")#
#
summary(avt.group2)		#
#
## Fit is ok: chisq = 38.5 with 31 df, p = .167. The fit is worst for Group 1 (chisq = 18.2). #
## But is the fit sig worse than when all params vary across groups?#
## Ans: The constrained model is not sig worse, and it has a lower AIC. So prefer it!#
#
cat('\n\n Comparison of unconstrained vs constrained group fits of Model2 \n\n')#
anova(avt.group1, avt.group2)#
cat('\n Unconstrained Model: ')#
print(c(AIC = AIC(avt.group1)))	#
cat('\n Constrained Model: ')#
print(fitMeasures(avt.group2, "AIC"))	#
#
## The sample sizes in this data set are small for SEM, and this is one reason why different#
## models are not significantly different - not enough power to make the distinctions.#
## Let us continue with model refinement purely to see how this might be done in the general case.#
#
## We can start with the unconstrained model, and then impose equality constraints on params.#
## Or we can start with the constrained model, and then relax the equality constraints.#
#
# Impose equality constraints in unconstrained model#
#
avt.model3 = '#
	# regressions#
	# X + Y ~ U + V + W#
	# Effect of cultatt on ideal hap, and on actual hap, same across cultures#
	# Effect of temperatt on ideal hap, and on actual hap, different across cultures#
	# All other params differ across cultures#
	ideahap ~ c(k1, k1, k1)*cultatt + c(l1, l2, l3)*temperatt#
	actuhap ~ c(k4, k4, k4)*cultatt + c(l4, l5, l6)*temperatt#
	rigoract ~ ideahap + temperatt#
	depress ~ actuhap + temperatt#
	# variances and covariances#
	# X ~~ Y#
	rigoract ~~ 0*depress#
	ideahap ~~ actuhap#
'#
#
avt.group3 = sem(avt.model3, fixed.x = F, data = d, group = "group", meanstructure = F)#
#
cat('\n\n Comparison of unconstrained vs semi-constrained group fits of Model2 \n\n')#
print(anova(avt.group1, avt.group3))
HW #7#
## M. Lewis#
## 4 June 2013#
#
library(lavaan)#
#A1#
#
lower = '#
.183#
.007 .500#
.009 .001 .035#
.556 .207 .018 13.025#
.061 .010 -.007 .466 .164#
.030 -.275 .096 .899 .216 25.646#
.188 .134 .015 .695 .060 -.163 .321'#
#
#- M1  -##
labels1 = c("EarlyOn", "COMT", "Schizo", "Psysym", "Conduct", "Chpsyq", "GenxEnv")#
comt7.cov = getCov(lower, names = labels1)#
#
comt7.model1 = '#
	Schizo ~ EarlyOn + GenxEnv + Psysym#
	Psysym ~ EarlyOn + Conduct + Schizo#
	Conduct ~ EarlyOn#
	Chpsyq ~ COMT#
'#
#
comt7.fit1 = sem(comt7.model1, fixed.x = F, sample.cov = comt7.cov, sample.nobs = 803)#
summary(comt7.fit1)#
modificationIndices(comt7.fit1) #
#
#- M2  -##
comt7.model2 = '#
	Schizo ~ EarlyOn + GenxEnv + Psysym + Chpsyq#
	Psysym ~ EarlyOn + Conduct + Schizo#
	Conduct ~ EarlyOn + Chpsyq #
	Chpsyq ~ COMT#
'#
comt7.fit2 = sem(comt7.model2, fixed.x = F, sample.cov = comt7.cov, sample.nobs = 803)#
#
summary(comt7.fit2)#
#
modificationIndices(comt7.fit2) #
#
#- M3  -##
#
comt7.model3 = '#
	Schizo ~ EarlyOn + GenxEnv + Psysym + Chpsyq#
	Psysym ~ EarlyOn + Conduct + Schizo#
	Conduct ~ EarlyOn + Chpsyq #
	Chpsyq ~ COMT#
	GenxEnv ~ Chpsyq#
	EarlyOn ~~ GenxEnv + COMT#
	GenxEnv ~~ COMT#
'#
comt7.fit3 = sem(comt7.model3, fixed.x = F, sample.cov = comt7.cov, sample.nobs = 803)#
#
summary(comt7.fit3)#
#
modificationIndices(comt7.fit3) #
#
anova(comt7.fit1, comt7.fit2, comt7.fit3) #
#A2#
#
lower5 = '#
.183#
.009 .035#
.556 .018 13.025#
.061 -.007 .466 .164#
.188 .015 .695 .060 .321#
'#
#
labels2 = c("EarlyOn", "Schizo", "Psysym", "Conduct", "GenxEnv")#
comt5.cov = getCov(lower5, names = labels2)#
#
#- M1  -##
# Specify the model#
comt5.model1 = '#
	Schizo ~ EarlyOn + GenxEnv + Psysym#
	Psysym ~ Conduct + Schizo#
	Conduct ~ EarlyOn#
'#
#
# Fit the model#
comt5.fit1 = sem(comt5.model1, fixed.x = F, sample.cov = comt5.cov, sample.nobs = 803)#
#
# min chi-sq#
comt5.fit1#
#
#- M2  -##
## Try to find a better model by inspecting the correl matrix and lm(psysym ~ .), lm(conduct ~ .)#
#
comt5.model2 = '#
	Schizo ~ Conduct + GenxEnv  #
	Psysym ~ EarlyOn + Conduct + GenxEnv #
	Conduct ~ EarlyOn #
'#
# Fit the model#
comt5.fit2 = sem(comt5.model2, fixed.x = F, sample.cov = comt5.cov, sample.nobs = 803)#
#
# min chi-sq#
print(comt5.fit2)		# chisq = 1.54 with 2 df, p = .464, an excellent fit! Examine parameter values#
print(summary(comt5.fit2))#
#- M3  -##
## Try to find a better model by inspecting the correl matrix and lm(psysym ~ .), lm(conduct ~ .)#
#
comt5.model3 = '#
	Schizo ~ Conduct + GenxEnv  #
	Psysym ~ EarlyOn + Conduct + GenxEnv #
	Conduct ~ EarlyOn #
	Schizo~~ 0*Psysym  #
	GenxEnv ~~ EarlyOn#
'#
# Fit the model#
comt5.fit3 = sem(comt5.model3, fixed.x = F, sample.cov = comt5.cov, sample.nobs = 803)#
#
# min chi-sq#
print(comt5.fit3)		# chisq = 1.54 with 2 df, p = .464, an excellent fit! Examine parameter values#
#
anova(comt5.fit1, comt5.fit2, comt5.fit3) #
#
#B1#
low.dom = '#
1#
.415 1#
.460 .351 1#
-.321 -.374 -.310 1#
-.239 -.221 -.133 .626 1#
-.185 -.164 -.272 .533 .345 1#
.349 .307 .496 -.180 -.081 -.067 1#
.308 .302 .562 -.222 -.156 -.118 .573 1#
.038 -.115 -.048 .296 .167 .320 .008 -.036 1#
-.072 -.160 -.124 .317 .167 .248 -.152 -.175 .414 1 #
'#
labels1 = c("FSOR", "FSIR", "FSSR", "MSOR", "MSIR", "MSSR", "FTFR", "FTSR", "MTSR", "MTFR")#
dom.cov = getCov(low.dom, names = labels1)#
#
dyad.model1 = '#
	# Latent variable definitions for trait, FTZ & MTZ, and situation, FSZ & MSZ	#
	FTZ =~ FTSR + FTFR#
	MTZ =~ MTSR + MTFR#
	FSZ =~ FSSR + FSIR + FSOR#
	MSZ =~ MSSR + MSIR + MSOR#
	# regressions#
	FSZ ~ B*FTZ + C*MSZ#
	MSZ ~ A*MTZ + D*FSZ#
	# variances and covariances#
	# Residual correls among M ratings, MTSR, MSSR, FSIR#
	MTSR + MSSR ~~ FSIR#
	MSSR ~~ MTSR#
	# Residual correls among F ratings, FTSR, FSSR, MSIR#
	FTSR + FSSR ~~ MSIR#
	FSSR ~~ FTSR#
	# Residual correls among Observer situational ratings, MSOR, FSOR	#
	MSOR ~~ FSOR	#
'#
#
dyad.fit1 = sem(dyad.model1, fixed.x = F, sample.cov = dom.cov, sample.nobs = 112)#
dyad.fit1 #good p value, but can we make a simpler model that works?#
#
#Taking out most of the covariance terms#
dyad.model2 = '#
	# Latent variable definitions for trait, FTZ & MTZ, and situation, FSZ & MSZ	#
	FTZ =~ FTSR + FTFR#
	MTZ =~ MTSR + MTFR#
	FSZ =~ FSSR + FSIR + FSOR#
	MSZ =~ MSSR + MSIR + MSOR#
	# regressions#
	FSZ ~ B*FTZ + C*MSZ#
	MSZ ~ A*MTZ + D*FSZ#
	# Residual correls among M ratings, MTSR, MSSR#
	MSSR ~~ MTSR#
	# Residual correls among F ratings, FTSR, FSSR#
	FSSR ~~ FTSR#
'#
dyad.fit2 = sem(dyad.model2, fixed.x = T, sample.cov = dom.cov, sample.nobs = 112) #notice how i set fixed.x = T here so it wouldn't automatically add terms I didn't want#
summary(dyad.fit2) # Wow, even better !#
#
anova(dyad.fit1,dyad.fit2) #Our simpler model is definitely the better one#
#
#Let's see if we need our "C" and "D" paths to be different (1 less parameter if we constrain them to be the same)#
#
dyad.model3 = '#
	# Latent variable definitions for trait, FTZ & MTZ, and situation, FSZ & MSZ	#
	FTZ =~ FTSR + FTFR#
	MTZ =~ MTSR + MTFR#
	FSZ =~ FSSR + FSIR + FSOR#
	MSZ =~ MSSR + MSIR + MSOR#
	# regressions#
	FSZ ~ B*FTZ + C*MSZ#
	MSZ ~ A*MTZ + C*FSZ#
	# Residual correls among M ratings, MTSR, MSSR#
	MSSR ~~ MTSR#
	# Residual correls among F ratings, FTSR, FSSR#
	FSSR ~~ FTSR#
'#
dyad.fit3 = sem(dyad.model3, fixed.x = T, sample.cov = dom.cov, sample.nobs = 112) #notice how i set fixed.x = T here so it wouldn't automatically add terms I didn't want#
summary(dyad.fit3)#
#
anova(dyad.fit1,dyad.fit2,dyad.fit3) #looks like constraining it made the model better! So they probably aren't actually different#
#
#could also test it this way:#
dyad.fit3a = sem(dyad.model2, fixed.x = F, sample.cov = dom.cov, sample.nobs = 112, constraints = 'C - D == 0', start = dyad.fit2) #using our model2 design, but constraining C-D to be 0#
print(summary(dyad.fit3a)) #same result as before#
#
#B2#
low.affil = '#
1#
.403 1#
.321 .159 1#
.317 .291 .116 1#
.166 .156 .689 .212 1#
.190 .722 .063 .269 .099 1#
.190 .132 .074 .073 .034 -.067 1#
.253 .085 .498 .065 .379 -.086 .198 1#
.092 .279 .054 .109 .068 .282 -.117 -.014 1#
.093 .145 .027 .123 .128 .150 -.046 .087 .182 1 #
'#
labels1 = c("FSOR", "FSIR", "FSSR", "MSOR", "MSIR", "MSSR", "FTFR", "FTSR", "MTSR", "MTFR")#
affil.cov = getCov(low.affil, names = labels1)#
dyad.fitA1 = sem(dyad.model1, fixed.x = F, sample.cov = affil.cov, sample.nobs = 112)#
dyad.fitA1#
summary(dyad.fitA1)#
#
dyad.fitA2 = sem(dyad.model2, fixed.x = T, sample.cov = affil.cov, sample.nobs = 112) #
dyad.fitA2#
summary(dyad.fitA2) #
#
dyad.fitA3 = sem(dyad.model3, fixed.x = T, sample.cov = affil.cov, sample.nobs = 112) #
dyad.fitA3#
summary(dyad.fitA3)#
#C1#
# Use raw data, jt-data1.csv, to test AVT#
#
d = read.csv("/Documents/GRADUATE_SCHOOL/Coursework/Psych. 253/HW7/jt-data1.csv")#
#
# Specify the model for whole sample, ignoring 'group'#
## M1###
#
avt.model1 = '#
	# regressions#
	# X + Y ~ U + V + W#
	ideahap + actuhap ~ cultatt + temperatt#
	rigoract + depress ~ ideahap + actuhap + cultatt + temperatt#
	# variances and covariances#
	# X ~~ Y#
	rigoract ~~ 0*depress#
'#
# Fit the model#
avt.fit1 = sem(avt.model1, fixed.x = F, data = d)#
avt.fit1#
summary(avt.fit1)	# #
#
# Model 1 is almost saturated - only 2 df is left for testing - yet it is poor.#
# This suggests that many of the paths are unhelpful, i.e., have coeffs near 0.#
# These unhelpful paths shd be removed and replaced by useful paths.#
# I was surprised that sem() estimated cov(rigoract, depress), because these vars are not #
# exogenous. It appears that sem(), by default, puts a cov path between any pair of variables#
# that are not already linked in the model! This is a waste of params. #
# So explicitly set these unwanted cov paths equal to 0. #
# A common finding in this field is that ideal HAP and actual HAP are correlated.#
# So introduce a cov link between them.#
## M2###
#
avt.model2 = '#
	# regressions#
	# X + Y ~ U + V + W#
	ideahap + actuhap ~ cultatt + temperatt#
	rigoract ~ ideahap + temperatt#
	depress ~ actuhap + temperatt#
	# variances and covariances#
	# X ~~ Y#
	rigoract ~~ 0*depress#
	ideahap ~~ actuhap#
'#
# Fit the model#
avt.fit2 = sem(avt.model2, fixed.x = F, data = d)#
summary(avt.fit2)#
#
## Multiple Group analyses: Does Model 2 apply to any one of the 3 groups, EA, AA and CH?#
#
grplab1 = c("EA", "AA", "CH")#
#
# Fit Model 2 to all 3 groups, params differ across groups#
#
avt.group1 = sem(avt.model2, fixed.x = F, data = d, group = "group", meanstructure = F)#
#
summary(avt.group1)		#
#
## The fit to each group is acceptable, except that the fit to Group 1 is almost sig#
## chisq = 9.8 with 5 df, p = .08.#
## The parameters were free to vary across groups. What if we were to equate params across groups?#
#
# Fit Model 2 with the SAME regression parameters to all 3 groups#
#
avt.group2 = sem(avt.model2, fixed.x = F, data = d, group = "group", meanstructure = F, group.equal = "regressions")#
#
summary(avt.group2)		#
#
## Fit is ok: chisq = 38.5 with 31 df, p = .167. The fit is worst for Group 1 (chisq = 18.2). #
## But is the fit sig worse than when all params vary across groups?#
## Ans: The constrained model is not sig worse, and it has a lower AIC. So prefer it!#
#
cat('\n\n Comparison of unconstrained vs constrained group fits of Model2 \n\n')#
anova(avt.group1, avt.group2)#
cat('\n Unconstrained Model: ')#
print(c(AIC = AIC(avt.group1)))	#
cat('\n Constrained Model: ')#
print(fitMeasures(avt.group2, "AIC"))	#
#
## The sample sizes in this data set are small for SEM, and this is one reason why different#
## models are not significantly different - not enough power to make the distinctions.#
## Let us continue with model refinement purely to see how this might be done in the general case.#
#
## We can start with the unconstrained model, and then impose equality constraints on params.#
## Or we can start with the constrained model, and then relax the equality constraints.#
#
# Impose equality constraints in unconstrained model#
#
avt.model3 = '#
	# regressions#
	# X + Y ~ U + V + W#
	# Effect of cultatt on ideal hap, and on actual hap, same across cultures#
	# Effect of temperatt on ideal hap, and on actual hap, different across cultures#
	# All other params differ across cultures#
	ideahap ~ c(k1, k1, k1)*cultatt + c(l1, l2, l3)*temperatt#
	actuhap ~ c(k4, k4, k4)*cultatt + c(l4, l5, l6)*temperatt#
	rigoract ~ ideahap + temperatt#
	depress ~ actuhap + temperatt#
	# variances and covariances#
	# X ~~ Y#
	rigoract ~~ 0*depress#
	ideahap ~~ actuhap#
'#
#
avt.group3 = sem(avt.model3, fixed.x = F, data = d, group = "group", meanstructure = F)#
#
cat('\n\n Comparison of unconstrained vs semi-constrained group fits of Model2 \n\n')#
print(anova(avt.group1, avt.group3))#
#
# Relax equality constraints in constrained model#
# Allow effect of temperatt on ideahap and on actuhap to vary across cultures#
#
avt.group4 = sem(avt.model2, fixed.x = F, data = d, group = "group", meanstructure = F, group.equal = "regressions", group.partial = c("ideahap ~ temperatt", "actuhap ~ temperatt"))#
#
cat('\n\n Comparison of fully constrained vs semi-constrained group fits of Model2 \n\n')#
print(anova(avt.group2, avt.group4))#
sink(file = NULL, append = F)#
#C2
source("/Documents/CaLL/from work computer/Experiments/Activity/Data_Organized/Experiment #6/Data/Activity6Analysis.R")
library('plyr')
library('plyr')#
  setwd('/Documents/GRADUATE_SCHOOL/Projects/ref_complex/Papers/RC/analysis/')#
  raw <- read.csv("data/experiment/RefComplex37.results_A.csv")#
  # RT dataframe#
  # melt#
  n <- names(raw)#
  cols = c(n[grepl("train", n)])#
  cols = cols[1:40]#
  md <- melt(raw, id.vars=c("workerid"), measure.vars=cols)#
  md$trial <- as.numeric(matrix(lapply(str_split(md$variable,"_"),function(x) {x[3]})))#
  md$var1 <- matrix(lapply(str_split(md$variable,"_"),function(x) {x[1]}))#
  md$var2 <- matrix(lapply(str_split(md$variable,"_"),function(x) {x[2]}))#
  md$var <- paste(md$var1, md$var2, sep = "_")#
  md$variable <- NULL; md$var1 <- NULL; md$var2 <- NULL#
  md$seq <- with(md, ave(value, workerid, var, FUN = seq_along))#
  d = cast(workerid + seq + trial ~ var, data = md, value.var = "value")#
  d$Answer.train_NA <- NULL; d$seq <- NULL
library('plyr')#
  library(reshape2)
setwd('/Documents/GRADUATE_SCHOOL/Projects/ref_complex/Papers/RC/analysis/')#
  raw <- read.csv("data/experiment/RefComplex37.results_A.csv")#
  # RT dataframe#
  # melt#
  n <- names(raw)#
  cols = c(n[grepl("train", n)])#
  cols = cols[1:40]#
  md <- melt(raw, id.vars=c("workerid"), measure.vars=cols)#
  md$trial <- as.numeric(matrix(lapply(str_split(md$variable,"_"),function(x) {x[3]})))#
  md$var1 <- matrix(lapply(str_split(md$variable,"_"),function(x) {x[1]}))#
  md$var2 <- matrix(lapply(str_split(md$variable,"_"),function(x) {x[2]}))#
  md$var <- paste(md$var1, md$var2, sep = "_")#
  md$variable <- NULL; md$var1 <- NULL; md$var2 <- NULL#
  md$seq <- with(md, ave(value, workerid, var, FUN = seq_along))#
  d = cast(workerid + seq + trial ~ var, data = md, value.var = "value")#
  d$Answer.train_NA <- NULL; d$seq <- NULL
summary(raw[cols])
detach(package:reshape)
detach(package:'reshape')
?detatch
??detach
detach('package:reshape')
detach("package:reshape")
rm(list=ls())#
  library(reshape2)#
#
  setwd('/Documents/GRADUATE_SCHOOL/Projects/ref_complex/Papers/RC/analysis/')#
  raw <- read.csv("data/experiment/RefComplex37.results_A.csv")#
  # RT dataframe#
  # melt#
  n <- names(raw)#
  cols = c(n[grepl("train", n)])#
  cols = cols[1:40]#
  md <- melt(raw, id.vars=c("workerid"), measure.vars=cols)
rm(list=ls())#
  library('plyr')#
  library(reshape2)#
#
  setwd('/Documents/GRADUATE_SCHOOL/Projects/ref_complex/Papers/RC/analysis/')#
  raw <- read.csv("data/experiment/RefComplex37.results_A.csv")#
  # RT dataframe#
  # melt#
  n <- names(raw)#
  cols = c(n[grepl("train", n)])#
  cols = cols[1:40]#
  md <- melt(raw, id.vars=c("workerid"), measure.vars=cols)
head(md)
tail(md)
md$trial <- as.numeric(matrix(lapply(str_split(md$variable,"_"),function(x) {x[3]})))#
  md$var1 <- matrix(lapply(str_split(md$variable,"_"),function(x) {x[1]}))#
  md$var2 <- matrix(lapply(str_split(md$variable,"_"),function(x) {x[2]}))#
  md$var <- paste(md$var1, md$var2, sep = "_")#
  md$variable <- NULL; md$var1 <- NULL; md$var2 <- NULL#
  md$seq <- with(md, ave(value, workerid, var, FUN = seq_along))#
  d = cast(workerid + seq + trial ~ var, data = md, value.var = "value")#
  d$Answer.train_NA <- NULL; d$seq <- NULL
md <- melt(raw, id.vars=c("workerid"), measure.vars=cols)#
  md$trial <- as.numeric(matrix(lapply(str_split(md$variable,"_"),function(x) {x[3]})))#
  md$var1 <- matrix(lapply(str_split(md$variable,"_"),function(x) {x[1]}))#
  md$var2 <- matrix(lapply(str_split(md$variable,"_"),function(x) {x[2]}))#
  md$var <- paste(md$var1, md$var2, sep = "_")#
  md$variable <- NULL; md$var1 <- NULL; md$var2 <- NULL#
  md$seq <- with(md, ave(value, workerid, var, FUN = seq_along))#
  d = cast(workerid + seq + trial ~ var, data = md, value.var = "value")#
  d$Answer.train_NA <- NULL; d$seq <- NULL
md <- melt(raw, id.vars=c("workerid"), measure.vars=cols)
md$trial <- as.numeric(matrix(lapply(str_split(md$variable,"_"),function(x) {x[3]})))
library(stringr)
setwd('/Documents/GRADUATE_SCHOOL/Projects/ref_complex/Papers/RC/analysis/')#
  raw <- read.csv("data/experiment/RefComplex37.results_A.csv")#
  # RT dataframe#
  # melt#
  n <- names(raw)#
  cols = c(n[grepl("train", n)])#
  cols = cols[1:40]#
  md <- melt(raw, id.vars=c("workerid"), measure.vars=cols)
md$trial <- as.numeric(matrix(lapply(str_split(md$variable,"_"),function(x) {x[3]})))#
  md$var1 <- matrix(lapply(str_split(md$variable,"_"),function(x) {x[1]}))#
  md$var2 <- matrix(lapply(str_split(md$variable,"_"),function(x) {x[2]}))#
  md$var <- paste(md$var1, md$var2, sep = "_")#
  md$variable <- NULL; md$var1 <- NULL; md$var2 <- NULL#
  md$seq <- with(md, ave(value, workerid, var, FUN = seq_along))#
  d = cast(workerid + seq + trial ~ var, data = md, value.var = "value")#
  d$Answer.train_NA <- NULL; d$seq <- NULL
library('plyr')#
  library(reshape)#
  library(stringr)#
#
  setwd('/Documents/GRADUATE_SCHOOL/Projects/ref_complex/Papers/RC/analysis/')#
  raw <- read.csv("data/experiment/RefComplex37.results_A.csv")#
  # RT dataframe#
  # melt#
  n <- names(raw)#
  cols = c(n[grepl("train", n)])#
  cols = cols[1:40]#
  md <- melt(raw, id.vars=c("workerid"), measure.vars=cols)#
  md$trial <- as.numeric(matrix(lapply(str_split(md$variable,"_"),function(x) {x[3]})))#
  md$var1 <- matrix(lapply(str_split(md$variable,"_"),function(x) {x[1]}))#
  md$var2 <- matrix(lapply(str_split(md$variable,"_"),function(x) {x[2]}))#
  md$var <- paste(md$var1, md$var2, sep = "_")#
  md$variable <- NULL; md$var1 <- NULL; md$var2 <- NULL#
  md$seq <- with(md, ave(value, workerid, var, FUN = seq_along))#
  d = cast(workerid + seq + trial ~ var, data = md, value.var = "value")#
  d$Answer.train_NA <- NULL; d$seq <- NULL
summary(d)
rm(list=ls())#
  library('plyr')#
  library(reshape2)#
  library(stringr)#
#
  setwd('/Documents/GRADUATE_SCHOOL/Projects/ref_complex/Papers/RC/analysis/')#
  raw <- read.csv("data/experiment/RefComplex37.results_A.csv")#
  # RT dataframe#
  # melt#
  n <- names(raw)#
  cols = c(n[grepl("train", n)])#
  cols = cols[1:40]#
  md <- melt(raw, id.vars=c("workerid"), measure.vars=cols)
head(md)
tail(md)
rm(list=ls())#
  library('plyr')#
  library(reshape2)#
  library(stringr)#
#
  setwd('/Documents/GRADUATE_SCHOOL/Projects/ref_complex/Papers/RC/analysis/')#
  raw <- read.csv("data/experiment/RefComplex37.results_A.csv")#
  # RT dataframe#
  # melt#
  n <- names(raw)#
  cols = c(n[grepl("train", n)])#
  cols = cols[1:40]#
  md <- melt(raw, id.vars=c("workerid"), measure.vars=cols)#
  tail(md)
rm(list=ls())#
  library('plyr')#
  library(reshape2)#
  #library(stringr)#
#
  setwd('/Documents/GRADUATE_SCHOOL/Projects/ref_complex/Papers/RC/analysis/')#
  raw <- read.csv("data/experiment/RefComplex37.results_A.csv")#
  # RT dataframe#
  # melt#
  n <- names(raw)#
  cols = c(n[grepl("train", n)])#
  cols = cols[1:40]#
  md <- melt(raw, id.vars=c("workerid"), measure.vars=cols)#
  tail(md)
rm(list=ls())
library('plyr')#
  library(reshape2)#
  #library(stringr)#
#
  setwd('/Documents/GRADUATE_SCHOOL/Projects/ref_complex/Papers/RC/analysis/')#
  raw <- read.csv("data/experiment/RefComplex37.results_A.csv")#
  # RT dataframe#
  # melt#
  n <- names(raw)#
  cols = c(n[grepl("train", n)])#
  cols = cols[1:40]#
  md <- melt(raw, id.vars=c("workerid"), measure.vars=cols)
tail(md)
